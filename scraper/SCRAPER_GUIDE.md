# å¬å›æ•°æ®çˆ¬è™«ç³»ç»Ÿ

> å¤šå“ç‰Œã€å¤šå›½å®¶çš„å©´å¹¼å„¿é…æ–¹å¥¶ç²‰å¬å›æ•°æ®é‡‡é›†ç³»ç»Ÿ

---

## ğŸ“‹ ç³»ç»Ÿæ¦‚è¿°

æœ¬ç³»ç»Ÿæ”¯æŒä»å„ä¸ªå“ç‰Œçš„å®˜æ–¹æ¸ é“å’Œæ”¿åºœç›‘ç®¡å¹³å°æŠ“å–å¬å›æ•°æ®ï¼Œå¹¶è‡ªåŠ¨å†™å…¥é£ä¹¦å¤šç»´è¡¨æ ¼ã€‚

### æ”¯æŒçš„å“ç‰Œ

| å“ç‰Œ | ä¸­æ–‡å | å­å“ç‰Œ | æ”¯æŒå›½å®¶/åœ°åŒº |
|------|--------|--------|---------------|
| NestlÃ© | é›€å·¢ | SMA, ALFAMINO, NAN, BEBA | UK |
| Abbott | é›…åŸ¹ | Similac, Alimentum, EleCare, Go & Grow | US |
| Aptamil | çˆ±ä»–ç¾ | Aptamil, Aptamil Essensis, Aptamil Profutura | UK, DE, AU, CN |
| Feihe | é£é¹¤ | æ˜Ÿé£å¸†, è‡»ç¨š, è‡»çˆ±, è‡»é«˜ | CN |
| Friso | ç¾ç´ ä½³å„¿ | ç¾ç´ ä½³å„¿, çš‡å®¶ç¾ç´ ä½³å„¿, ç¾ç´ åŠ›, ä½³è´è‰¾ç‰¹ | CN, NL |
| a2 | a2è‡³åˆ | a2è‡³åˆ, a2 Platinum, a2 Smart Nutrition | CN, NZ, AU |
| Jinlingguan | é‡‘é¢†å†  | é‡‘é¢†å† , çæŠ¤, ç¿æŠ¤, è‚²æŠ¤ | CN |

### æ•°æ®æº

- **æ”¿åºœå¹³å°**ï¼šè‹±å›½ FSAã€ä¸­å›½ SAMRã€å¾·å›½ BVLã€æ–°è¥¿å…° MPIã€æ¾³å¤§åˆ©äºš FSANZã€è·å…° NVWA
- **å®˜æ–¹ç½‘ç«™**ï¼šå„å“ç‰Œå®˜ç½‘çš„å¬å›å…¬å‘Šé¡µé¢

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
scraper/
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ __init__.py              # æ¨¡å—åˆå§‹åŒ–
â”‚   â”œâ”€â”€ base_scraper.py          # çˆ¬è™«åŸºç±»
â”‚   â”œâ”€â”€ brand_config.py          # å“ç‰Œé…ç½®
â”‚   â”œâ”€â”€ aptamil_scraper.py       # çˆ±ä»–ç¾çˆ¬è™«
â”‚   â”œâ”€â”€ feihe_scraper.py         # é£é¹¤çˆ¬è™«
â”‚   â”œâ”€â”€ friso_scraper.py         # ç¾ç´ ä½³å„¿çˆ¬è™«
â”‚   â”œâ”€â”€ a2_scraper.py            # a2è‡³åˆçˆ¬è™«
â”‚   â”œâ”€â”€ jinlingguan_scraper.py   # é‡‘é¢†å† çˆ¬è™«
â”‚   â””â”€â”€ nestle_scraper.py        # é›€å·¢çˆ¬è™«ï¼ˆæ—§ç‰ˆï¼‰
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ feishu_config.py         # é£ä¹¦é…ç½®
â”‚   â”œâ”€â”€ feishu_client.py         # é£ä¹¦å®¢æˆ·ç«¯
â”‚   â””â”€â”€ feishu_tables.py         # é£ä¹¦è¡¨æ ¼ç®¡ç†
â”œâ”€â”€ run_scrapers.py              # ç»Ÿä¸€è¿è¡Œå™¨
â”œâ”€â”€ abbott_scraper.py            # é›…åŸ¹çˆ¬è™«ï¼ˆæ—§ç‰ˆï¼‰
â””â”€â”€ nestle_scraper.py            # é›€å·¢çˆ¬è™«ï¼ˆæ—§ç‰ˆï¼‰
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚

- Python 3.9+
- ä¾èµ–åŒ…ï¼šrequests, beautifulsoup4, PyPDF2

### 2. å®‰è£…ä¾èµ–

```bash
cd scraper
pip install -r requirements.txt
```

### 3. é…ç½®é£ä¹¦

ç¼–è¾‘ `utils/feishu_config.py`ï¼š

```python
APP_ID = "your_app_id"
APP_SECRET = "your_app_secret"
APP_TOKEN = "your_app_token"
TABLE_ID = "your_table_id"
```

### 4. è¿è¡Œçˆ¬è™«

#### æŸ¥çœ‹æ‰€æœ‰æ•°æ®æº

```bash
python run_scrapers.py --sources
```

#### è¿è¡Œå•ä¸ªå“ç‰Œ

```bash
python run_scrapers.py --brand aptamil
```

#### è¿è¡Œæ‰€æœ‰å“ç‰Œ

```bash
python run_scrapers.py --all
```

#### è¿è¡Œå¹¶æ’å…¥é£ä¹¦

```bash
python run_scrapers.py --all --insert
```

#### è¿è¡Œå¹¶ä¿å­˜ç»“æœ

```bash
python run_scrapers.py --all --save
```

---

## ğŸ“– ä½¿ç”¨æŒ‡å—

### å‘½ä»¤è¡Œå‚æ•°

| å‚æ•° | è¯´æ˜ |
|------|------|
| `--brand <brand_key>` | è¿è¡Œå•ä¸ªå“ç‰Œçš„çˆ¬è™«ï¼ˆå¦‚ï¼šaptamil, feihe, friso, a2, jinlingguanï¼‰ |
| `--all` | è¿è¡Œæ‰€æœ‰å“ç‰Œçš„çˆ¬è™« |
| `--insert` | å°†æŠ“å–çš„æ•°æ®æ’å…¥é£ä¹¦è¡¨æ ¼ |
| `--save` | ä¿å­˜æŠ“å–ç»“æœåˆ° JSON æ–‡ä»¶ |
| `--sources` | æ˜¾ç¤ºæ‰€æœ‰æ•°æ®æº |

### ç¤ºä¾‹

1. **æŠ“å–çˆ±ä»–ç¾çš„å¬å›æ•°æ®**

```bash
python run_scrapers.py --brand aptamil
```

2. **æŠ“å–é£é¹¤çš„å¬å›æ•°æ®å¹¶æ’å…¥é£ä¹¦**

```bash
python run_scrapers.py --brand feihe --insert
```

3. **æŠ“å–æ‰€æœ‰å“ç‰Œçš„å¬å›æ•°æ®**

```bash
python run_scrapers.py --all
```

4. **æŠ“å–æ‰€æœ‰å“ç‰Œã€æ’å…¥é£ä¹¦ã€ä¿å­˜ç»“æœ**

```bash
python run_scrapers.py --all --insert --save
```

---

## ğŸ”§ å¼€å‘æŒ‡å—

### åˆ›å»ºæ–°å“ç‰Œçˆ¬è™«

1. åœ¨ `scrapers/` ç›®å½•ä¸‹åˆ›å»ºæ–°çš„çˆ¬è™«æ–‡ä»¶ï¼ˆå¦‚ `mybrand_scraper.py`ï¼‰

2. ç»§æ‰¿ `BaseScraper` ç±»

```python
from .base_scraper import BaseScraper
from typing import List, Dict, Any

class MyBrandScraper(BaseScraper):
    def __init__(self):
        super().__init__("å“ç‰Œä¸­æ–‡å", "BrandName")

    def scrape(self) -> List[Dict[str, Any]]:
        # å®ç°æŠ“å–é€»è¾‘
        products = []
        # ... æŠ“å–ä»£ç  ...
        return products
```

3. åœ¨ `brand_config.py` ä¸­æ·»åŠ å“ç‰Œé…ç½®

```python
"mybrand": {
    "name": "å“ç‰Œä¸­æ–‡å",
    "name_en": "BrandName",
    "sub_brands": ["å­å“ç‰Œ1", "å­å“ç‰Œ2"],
    "sources": [
        {
            "country": "CN",
            "source_type": "æ”¿åºœå¹³å°",
            "url": "https://example.com/recalls",
            "parser": "my_parser"
        }
    ]
}
```

4. åœ¨ `run_scrapers.py` ä¸­æ³¨å†Œçˆ¬è™«

```python
from scrapers.mybrand_scraper import MyBrandScraper

class RecallScraperRunner:
    def __init__(self):
        self.scrapers = {
            # ... å…¶ä»–çˆ¬è™« ...
            "mybrand": MyBrandScraper
        }
```

### æ•°æ®æ ¼å¼

çˆ¬è™«è¿”å›çš„äº§å“æ•°æ®æ ¼å¼ï¼š

```python
{
    "product_name": "äº§å“åç§°",
    "sub_brand": "å­å“ç‰Œ",
    "pack_size": "800g",
    "batch_codes": ["æ‰¹æ¬¡å·1", "æ‰¹æ¬¡å·2"],
    "best_before": 1736841600,  # æ—¶é—´æˆ³
    "region": "CN",
    "recall_reason": "å¬å›åŸå› ",
    "risk_level": "é«˜",
    "source_type": "æ”¿åºœå¹³å°",
    "published_date": 1736841600,
    "status": "å¬å›ä¸­"
}
```

---

## ğŸ“Š é£ä¹¦è¡¨æ ¼å­—æ®µ

| å­—æ®µå | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| brand | æ–‡æœ¬ | å“ç‰Œï¼ˆé›€å·¢ã€Abbottç­‰ï¼‰ |
| brand_en | æ–‡æœ¬ | å“ç‰Œè‹±æ–‡å |
| product_name | æ–‡æœ¬ | äº§å“åç§° |
| sub_brand | æ–‡æœ¬ | å­å“ç‰Œï¼ˆSMAã€NANï¼‰ |
| batch_codes | æ–‡æœ¬ | æ‰¹æ¬¡å·ï¼ˆé€—å·åˆ†éš”ï¼‰ |
| pack_size | æ–‡æœ¬ | åŒ…è£…è§„æ ¼ï¼ˆ800gã€400gï¼‰ |
| best_before | æ—¥æœŸæ—¶é—´ | æœ‰æ•ˆæœŸ |
| region | æ–‡æœ¬ | å—å½±å“åœ°åŒº |
| recall_reason | æ–‡æœ¬ | å¬å›åŸå›  |
| risk_level | å•é€‰ | é£é™©ç­‰çº§ï¼ˆé«˜/ä¸­/ä½ï¼‰ |
| source_url | è¶…é“¾æ¥ | å®˜æ–¹æ¥æºé“¾æ¥ |
| source_type | å•é€‰ | æ•°æ®æºç±»å‹ |
| published_date | æ—¥æœŸæ—¶é—´ | å‘å¸ƒæ—¥æœŸ |
| last_updated | æ—¥æœŸæ—¶é—´ | æœ€åæ›´æ–°æ—¥æœŸ |
| status | å•é€‰ | çŠ¶æ€ï¼ˆå¬å›ä¸­/å·²ç»“æŸ/å¾…ç¡®è®¤ï¼‰ |

---

## ğŸ” å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆæ²¡æœ‰æŠ“åˆ°æ•°æ®ï¼Ÿ

A: å¯èƒ½çš„åŸå› ï¼š
1. ç½‘ç«™ç»“æ„å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦æ›´æ–°è§£æé€»è¾‘
2. ç½‘ç«™åçˆ¬è™«æœºåˆ¶ï¼Œéœ€è¦æ·»åŠ è¯·æ±‚å¤´æˆ–ä½¿ç”¨ä»£ç†
3. è¯¥å“ç‰Œå½“å‰æ²¡æœ‰å¬å›ä¿¡æ¯

### Q: å¦‚ä½•å¤„ç†åŠ¨æ€åŠ è½½çš„é¡µé¢ï¼Ÿ

A: ä½¿ç”¨ Playwright æˆ– Seleniumï¼š

```python
from playwright.sync_api import sync_playwright

def scrape_dynamic_page():
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto('https://example.com')
        # ç­‰å¾…é¡µé¢åŠ è½½
        page.wait_for_selector('.recall-item')
        html = page.content()
        # è§£æ HTML
        browser.close()
```

### Q: å¦‚ä½•é¿å…è¢«å°IPï¼Ÿ

A: å»ºè®®æ–¹æ³•ï¼š
1. æ·»åŠ éšæœºå»¶è¿Ÿ
2. ä½¿ç”¨ä»£ç†IPæ± 
3. è®¾ç½®åˆç†çš„è¯·æ±‚å¤´
4. æ§åˆ¶çˆ¬å–é¢‘ç‡

### Q: å¦‚ä½•è°ƒè¯•çˆ¬è™«ï¼Ÿ

A: æŸ¥çœ‹æ—¥å¿—å’Œæ‰“å°ä¿¡æ¯ï¼š

```python
# åœ¨çˆ¬è™«ä¸­æ·»åŠ è°ƒè¯•ä¿¡æ¯
print(f"æŠ“å–URL: {url}")
print(f"HTMLé•¿åº¦: {len(html)}")
print(f"æ‰¾åˆ°çš„äº§å“æ•°: {len(products)}")
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **éµå®ˆç½‘ç«™è§„åˆ™**ï¼šå°Šé‡ç½‘ç«™çš„ robots.txt å’Œä½¿ç”¨æ¡æ¬¾
2. **åˆç†è®¾ç½®é¢‘ç‡**ï¼šé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆè¿‡å¤§å‹åŠ›
3. **æ•°æ®éªŒè¯**ï¼šæŠ“å–åéªŒè¯æ•°æ®å‡†ç¡®æ€§
4. **å®šæœŸæ›´æ–°**ï¼šç½‘ç«™ç»“æ„å¯èƒ½å˜åŒ–ï¼Œéœ€è¦å®šæœŸç»´æŠ¤çˆ¬è™«
5. **å…è´£å£°æ˜**ï¼šæœ¬ç³»ç»Ÿä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæ³•å¾‹ä¾æ®

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç å’Œæå‡ºå»ºè®®ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

---

## ğŸ“„ è®¸å¯è¯

MIT License

---

**ç‰ˆæœ¬**ï¼šv2.0
**æœ€åæ›´æ–°**ï¼š2026-01-30
**ç»´æŠ¤è€…**ï¼šäº§å“+å¼€å‘å›¢é˜Ÿ
